{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eff52e1-849d-449e-aa95-9c69563c1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# If you're in a fresh environment, uncomment installs:\n",
    "# !pip install numpy pandas scikit-learn xgboost nltk bs4 fuzzywuzzy[speedup] python-Levenshtein gensim tensorflow matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Extra models\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Text utils\n",
    "from fuzzywuzzy import fuzz\n",
    "import distance\n",
    "\n",
    "# NLTK & stemming\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc04dd45-69e2-4269-898c-32cc4717686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>I am poor but I want to invest. What should I do?</td>\n",
       "      <td>I am quite poor and I want to be very rich. Wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>I am from India and live abroad. I met a guy f...</td>\n",
       "      <td>T.I.E.T to Thapar University to Thapar Univers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>Why do so many people in the U.S. hate the sou...</td>\n",
       "      <td>My boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>Consequences of Bhopal gas tragedy?</td>\n",
       "      <td>What was the reason behind the Bhopal gas trag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  398782  496695  532029  What is the best marketing automation tool for...   \n",
       "1  115086  187729  187730  I am poor but I want to invest. What should I do?   \n",
       "2  327711  454161  454162  I am from India and live abroad. I met a guy f...   \n",
       "3  367788  498109  491396  Why do so many people in the U.S. hate the sou...   \n",
       "4  151235  237843   50930                Consequences of Bhopal gas tragedy?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the best marketing automation tool for...             1  \n",
       "1  I am quite poor and I want to be very rich. Wh...             0  \n",
       "2  T.I.E.T to Thapar University to Thapar Univers...             0  \n",
       "3  My boyfriend doesnt feel guilty when he hurts ...             0  \n",
       "4  What was the reason behind the Bhopal gas trag...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data (same schema as Quora Kaggle)\n",
    "df = pd.read_csv('train_quora.csv')\n",
    "\n",
    "# Optional: speed up with a sample\n",
    "new_df = df.sample(400000, random_state=2).reset_index(drop=True)\n",
    "\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe212ba8-6bbe-4c07-8901-82b7b0bf2ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5adf180-3461-4f82-8b0d-5ab7c7a5e228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best market autom tool small mid size compani</td>\n",
       "      <td>best market autom tool small mid size compani</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poor want invest</td>\n",
       "      <td>quit poor want rich</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>india live abroad met guy franc parti want date</td>\n",
       "      <td>e thapar univers thapar univers institut engin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mani peopl u hate southern state</td>\n",
       "      <td>boyfriend doesnt feel guilti hurt cri tell kil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>consequ bhopal ga tragedi</td>\n",
       "      <td>reason behind bhopal ga tragedi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question1  \\\n",
       "0    best market autom tool small mid size compani   \n",
       "1                                 poor want invest   \n",
       "2  india live abroad met guy franc parti want date   \n",
       "3                 mani peopl u hate southern state   \n",
       "4                        consequ bhopal ga tragedi   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0      best market autom tool small mid size compani             1  \n",
       "1                                quit poor want rich             0  \n",
       "2  e thapar univers thapar univers institut engin...             0  \n",
       "3  boyfriend doesnt feel guilti hurt cri tell kil...             0  \n",
       "4                    reason behind bhopal ga tragedi             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decontract_and_clean(q: str) -> str:\n",
    "    q = str(q).lower().strip()\n",
    "    q = q.replace('%', ' percent').replace('$', ' dollar ').replace('₹', ' rupee ').replace('€', ' euro ').replace('@', ' at ')\n",
    "    q = q.replace('[math]', '')\n",
    "    q = q.replace(',000,000,000 ', 'b ').replace(',000,000 ', 'm ').replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "\n",
    "    contractions = {\n",
    "        \"ain't\":\"am not\",\"aren't\":\"are not\",\"can't\":\"can not\",\"can't've\":\"can not have\",\"'cause\":\"because\",\n",
    "        \"could've\":\"could have\",\"couldn't\":\"could not\",\"couldn't've\":\"could not have\",\"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\"don't\":\"do not\",\"hadn't\":\"had not\",\"hadn't've\":\"had not have\",\"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\"he'd\":\"he would\",\"he'd've\":\"he would have\",\"he'll\":\"he will\",\"he'll've\":\"he will have\",\n",
    "        \"he's\":\"he is\",\"how'd\":\"how did\",\"how'd'y\":\"how do you\",\"how'll\":\"how will\",\"how's\":\"how is\",\n",
    "        \"i'd\":\"i would\",\"i'd've\":\"i would have\",\"i'll\":\"i will\",\"i'll've\":\"i will have\",\"i'm\":\"i am\",\"i've\":\"i have\",\n",
    "        \"isn't\":\"is not\",\"it'd\":\"it would\",\"it'd've\":\"it would have\",\"it'll\":\"it will\",\"it'll've\":\"it will have\",\n",
    "        \"it's\":\"it is\",\"let's\":\"let us\",\"ma'am\":\"madam\",\"mayn't\":\"may not\",\"might've\":\"might have\",\"mightn't\":\"might not\",\n",
    "        \"mightn't've\":\"might not have\",\"must've\":\"must have\",\"mustn't\":\"must not\",\"mustn't've\":\"must not have\",\n",
    "        \"needn't\":\"need not\",\"needn't've\":\"need not have\",\"o'clock\":\"of the clock\",\"oughtn't\":\"ought not\",\n",
    "        \"oughtn't've\":\"ought not have\",\"shan't\":\"shall not\",\"sha'n't\":\"shall not\",\"shan't've\":\"shall not have\",\n",
    "        \"she'd\":\"she would\",\"she'd've\":\"she would have\",\"she'll\":\"she will\",\"she'll've\":\"she will have\",\"she's\":\"she is\",\n",
    "        \"should've\":\"should have\",\"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"so've\":\"so have\",\"so's\":\"so as\",\n",
    "        \"that'd\":\"that would\",\"that'd've\":\"that would have\",\"that's\":\"that is\",\"there'd\":\"there would\",\"there'd've\":\"there would have\",\n",
    "        \"there's\":\"there is\",\"they'd\":\"they would\",\"they'd've\":\"they would have\",\"they'll\":\"they will\",\"they'll've\":\"they will have\",\n",
    "        \"they're\":\"they are\",\"they've\":\"they have\",\"to've\":\"to have\",\"wasn't\":\"was not\",\"we'd\":\"we would\",\"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\"we'll've\":\"we will have\",\"we're\":\"we are\",\"we've\":\"we have\",\"weren't\":\"were not\",\n",
    "        \"what'll\":\"what will\",\"what'll've\":\"what will have\",\"what're\":\"what are\",\"what's\":\"what is\",\"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\"when've\":\"when have\",\"where'd\":\"where did\",\"where's\":\"where is\",\"where've\":\"where have\",\n",
    "        \"who'll\":\"who will\",\"who'll've\":\"who will have\",\"who's\":\"who is\",\"who've\":\"who have\",\"why's\":\"why is\",\"why've\":\"why have\",\n",
    "        \"will've\":\"will have\",\"won't\":\"will not\",\"won't've\":\"will not have\",\"would've\":\"would have\",\"wouldn't\":\"would not\",\n",
    "        \"wouldn't've\":\"would not have\",\"y'all\":\"you all\",\"y'all'd\":\"you all would\",\"y'all'd've\":\"you all would have\",\n",
    "        \"y'all're\":\"you all are\",\"y'all've\":\"you all have\",\"you'd\":\"you would\",\"you'd've\":\"you would have\",\n",
    "        \"you'll\":\"you will\",\"you'll've\":\"you will have\",\"you're\":\"you are\",\"you've\":\"you have\"\n",
    "    }\n",
    "    q = ' '.join([contractions.get(w, w) for w in q.split()])\n",
    "    q = q.replace(\"'ve\",\" have\").replace(\"n't\",\" not\").replace(\"'re\",\" are\").replace(\"'ll\",\" will\")\n",
    "\n",
    "    q = BeautifulSoup(q, \"html.parser\").get_text()\n",
    "    q = re.sub(r'\\W', ' ', q).strip()\n",
    "    return q\n",
    "\n",
    "def stem_text(q: str) -> str:\n",
    "    return ' '.join(STEMMER.stem(w) for w in q.split() if w and w not in STOP_WORDS)\n",
    "\n",
    "# Apply preprocess + stemming\n",
    "new_df['question1'] = new_df['question1'].apply(lambda x: stem_text(decontract_and_clean(x)))\n",
    "new_df['question2'] = new_df['question2'].apply(lambda x: stem_text(decontract_and_clean(x)))\n",
    "\n",
    "new_df[['question1','question2','is_duplicate']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1807317-4214-475c-8049-aca194e59b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 21), (400000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def common_words(q1, q2):\n",
    "    w1 = set(q1.split()); w2 = set(q2.split())\n",
    "    return len(w1 & w2)\n",
    "\n",
    "def total_words(q1, q2):\n",
    "    w1 = set(q1.split()); w2 = set(q2.split())\n",
    "    return len(w1) + len(w2)\n",
    "\n",
    "def token_features(q1, q2):\n",
    "    SAFE_DIV = 1e-4\n",
    "    q1_t, q2_t = q1.split(), q2.split()\n",
    "    if not q1_t or not q2_t: return [0.]*8\n",
    "\n",
    "    q1_words = set([w for w in q1_t if w not in STOP_WORDS])\n",
    "    q2_words = set([w for w in q2_t if w not in STOP_WORDS])\n",
    "    q1_stops = set([w for w in q1_t if w in STOP_WORDS])\n",
    "    q2_stops = set([w for w in q2_t if w in STOP_WORDS])\n",
    "\n",
    "    c_words = len(q1_words & q2_words)\n",
    "    c_stops = len(q1_stops & q2_stops)\n",
    "    c_tokens = len(set(q1_t) & set(q2_t))\n",
    "\n",
    "    feats = [0.]*8\n",
    "    feats[0] = c_words / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    feats[1] = c_words / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    feats[2] = c_stops / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    feats[3] = c_stops / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    feats[4] = c_tokens / (min(len(q1_t), len(q2_t)) + SAFE_DIV)\n",
    "    feats[5] = c_tokens / (max(len(q1_t), len(q2_t)) + SAFE_DIV)\n",
    "    feats[6] = int(q1_t[-1] == q2_t[-1])\n",
    "    feats[7] = int(q1_t[0] == q2_t[0])\n",
    "    return feats\n",
    "\n",
    "def length_features(q1, q2):\n",
    "    q1_t, q2_t = q1.split(), q2.split()\n",
    "    if not q1_t or not q2_t: return [0.,0.,0.]\n",
    "    f0 = abs(len(q1_t) - len(q2_t))\n",
    "    f1 = (len(q1_t) + len(q2_t)) / 2\n",
    "    lcs = list(distance.lcsubstrings(q1, q2))\n",
    "    f2 = (len(lcs[0]) / (min(len(q1), len(q2)) + 1.0)) if lcs else 0.0\n",
    "    return [f0, f1, f2]\n",
    "\n",
    "def fuzzy_features(q1, q2):\n",
    "    return [\n",
    "        fuzz.QRatio(q1, q2),\n",
    "        fuzz.partial_ratio(q1, q2),\n",
    "        fuzz.token_sort_ratio(q1, q2),\n",
    "        fuzz.token_set_ratio(q1, q2),\n",
    "    ]\n",
    "\n",
    "# Build engineered feature matrix\n",
    "feats = []\n",
    "for q1, q2 in zip(new_df['question1'], new_df['question2']):\n",
    "    row = []\n",
    "    row += [len(q1), len(q2), len(q1.split()), len(q2.split()),\n",
    "            common_words(q1, q2), total_words(q1, q2)]\n",
    "    row += token_features(q1, q2)\n",
    "    row += length_features(q1, q2)\n",
    "    row += fuzzy_features(q1, q2)\n",
    "    feats.append(row)\n",
    "\n",
    "feat_cols = [\n",
    "    'q1_len','q2_len','q1_num_words','q2_num_words','word_common','word_total',\n",
    "    'cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq',\n",
    "    'abs_len_diff','mean_len','longest_substr_ratio',\n",
    "    'fuzz_ratio','fuzz_partial_ratio','token_sort_ratio','token_set_ratio'\n",
    "]\n",
    "engineered_X = pd.DataFrame(feats, columns=feat_cols, index=new_df.index)\n",
    "y = new_df['is_duplicate'].values\n",
    "\n",
    "engineered_X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcdf2b79-25d2-47b1-a4ec-424048c7529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered: (400000, 21)\n",
      "TF-IDF: (400000, 8000)\n",
      "Combined: (400000, 8021)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Prepare raw texts\n",
    "q1_texts = new_df['question1'].tolist()\n",
    "q2_texts = new_df['question2'].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# CountVectorizer\n",
    "# -------------------------------\n",
    "cv = CountVectorizer(max_features=4000, ngram_range=(1,2))\n",
    "q1_bow = cv.fit_transform(q1_texts)\n",
    "q2_bow = cv.transform(q2_texts)\n",
    "\n",
    "# Keep sparse, don't use .toarray()\n",
    "bow_X = hstack([q1_bow, q2_bow])\n",
    "\n",
    "# -------------------------------\n",
    "# TF-IDF\n",
    "# -------------------------------\n",
    "tfidf = TfidfVectorizer(max_features=4000, ngram_range=(1,2))\n",
    "q1_tfidf = tfidf.fit_transform(q1_texts)\n",
    "q2_tfidf = tfidf.transform(q2_texts)\n",
    "\n",
    "# Keep sparse\n",
    "tfidf_X = hstack([q1_tfidf, q2_tfidf])\n",
    "\n",
    "# -------------------------------\n",
    "# Combine engineered features + TF-IDF\n",
    "# Convert engineered features (pandas DataFrame) to sparse first\n",
    "# -------------------------------\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "engineered_sparse = csr_matrix(engineered_X.values)\n",
    "\n",
    "# Combine (engineered + tfidf)\n",
    "combined_X = hstack([engineered_sparse, tfidf_X])\n",
    "\n",
    "# Check shapes\n",
    "print(\"Engineered:\", engineered_X.shape)\n",
    "print(\"TF-IDF:\", tfidf_X.shape)\n",
    "print(\"Combined:\", combined_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5355d8-0f68-4de9-9b99-53fa6fed0f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320000, 8021), (80000, 8021))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3740bb2a-583f-4b82-931f-66f45ad7e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params: {'C': 0.5}\n",
      "Best CV F1: 0.7073613273500656\n",
      "LinearSVC Accuracy: 0.79325\n",
      "LinearSVC F1: 0.7115854083839018\n",
      "[[43056  7404]\n",
      " [ 9136 20404]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Faster SVM variant for text data\n",
    "svc = LinearSVC(max_iter=2000, random_state=42)\n",
    "\n",
    "# Only C is tuned (kernel & gamma not needed for LinearSVC)\n",
    "param_grid = {\n",
    "    'C': [0.5, 1, 2, 5]\n",
    "}\n",
    "\n",
    "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(svc, param_grid, scoring='f1', cv=cv5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)\n",
    "\n",
    "# Best model\n",
    "svm_best = grid.best_estimator_\n",
    "pred = svm_best.predict(X_test)\n",
    "\n",
    "print(\"LinearSVC Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"LinearSVC F1:\", f1_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa186c33-1a90-4ee5-912c-3f03b43dd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg CV F1: 0.6957645936184196\n",
      "LogReg Test F1: 0.6973252652810664\n",
      "XGB Acc: 0.798775\n",
      "XGB F1 : 0.718841693446975\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on combined features\n",
    "logit = LogisticRegression(max_iter=2000, n_jobs=-1 if hasattr(LogisticRegression,'n_jobs') else None)\n",
    "scores = cross_val_score(logit, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)\n",
    "print(\"LogReg CV F1:\", scores.mean())\n",
    "logit.fit(X_train, y_train)\n",
    "print(\"LogReg Test F1:\", f1_score(y_test, logit.predict(X_test)))\n",
    "\n",
    "# XGBoost quick baseline\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.1,\n",
    "    subsample=0.8, colsample_bytree=0.8, eval_metric='logloss', n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "print(\"XGB Acc:\", accuracy_score(y_test, pred_xgb))\n",
    "print(\"XGB F1 :\", f1_score(y_test, pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35964839-3c19-434e-8112-0c28d2e6fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-W2V + Engineered — LogReg Acc: 0.7574125\n",
      "TFIDF-W2V + Engineered — LogReg F1 : 0.6563368808769102\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model on both q1 & q2 tokens\n",
    "sentences = [q.split() for q in (q1_texts + q2_texts)]\n",
    "w2v_model = Word2Vec(sentences, vector_size=200, window=5, min_count=2, workers=4, sg=1, epochs=10)\n",
    "\n",
    "# Helper: average Word2Vec for a sentence\n",
    "def sentence_avg_w2v(sent, model, size=200):\n",
    "    words = [w for w in sent.split() if w in model.wv]\n",
    "    if not words: return np.zeros(size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "# Build Avg W2V features\n",
    "q1_w2v = np.vstack([sentence_avg_w2v(s, w2v_model, 200) for s in q1_texts])\n",
    "q2_w2v = np.vstack([sentence_avg_w2v(s, w2v_model, 200) for s in q2_texts])\n",
    "avgw2v_X = np.hstack([q1_w2v, q2_w2v])\n",
    "\n",
    "# TF-IDF weights (from the TF-IDF vectorizer built earlier)\n",
    "idf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "def sentence_tfidf_w2v(sent, model, idf_dict, size=200):\n",
    "    words = [w for w in sent.split() if w in model.wv]\n",
    "    if not words:\n",
    "        return np.zeros(size)\n",
    "    weights, vecs = [], []\n",
    "    for w in words:\n",
    "        weights.append(idf_dict.get(w, 1.0))\n",
    "        vecs.append(model.wv[w])\n",
    "    weights = np.array(weights)\n",
    "    vecs = np.array(vecs)\n",
    "    return np.average(vecs, axis=0, weights=weights)\n",
    "\n",
    "q1_tw2v = np.vstack([sentence_tfidf_w2v(s, w2v_model, idf, 200) for s in q1_texts])\n",
    "q2_tw2v = np.vstack([sentence_tfidf_w2v(s, w2v_model, idf, 200) for s in q2_texts])\n",
    "tfidf_w2v_X = np.hstack([q1_tw2v, q2_tw2v])\n",
    "\n",
    "# Combine engineered + tfidf_w2v (often strong)\n",
    "X_mix_w2v = np.hstack([engineered_X.values, tfidf_w2v_X])\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_mix_w2v, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "logit2 = LogisticRegression(max_iter=2000)\n",
    "logit2.fit(X_train_w2v, y_train_w2v)\n",
    "pred2 = logit2.predict(X_test_w2v)\n",
    "print(\"TFIDF-W2V + Engineered — LogReg Acc:\", accuracy_score(y_test_w2v, pred2))\n",
    "print(\"TFIDF-W2V + Engineered — LogReg F1 :\", f1_score(y_test_w2v, pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3991e28f-eb0c-46f9-b79e-9ca9c1da77ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, (400000, 30), (400000, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a shared tokenizer over both questions\n",
    "all_texts = q1_texts + q2_texts\n",
    "max_words = 40000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "seq_q1 = tokenizer.texts_to_sequences(q1_texts)\n",
    "seq_q2 = tokenizer.texts_to_sequences(q2_texts)\n",
    "\n",
    "max_len = 30  # adjust if you want\n",
    "X1 = pad_sequences(seq_q1, maxlen=max_len, padding='post', truncating='post')\n",
    "X2 = pad_sequences(seq_q2, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "X_train_nn_q1, X_test_nn_q1, y_train_nn, y_test_nn = train_test_split(X1, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_nn_q2, X_test_nn_q2, _, _ = train_test_split(X2, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "vocab_size, X1.shape, X2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "686e2e1b-e377-4efb-9273-c1d833625247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 80ms/step - accuracy: 0.7081 - loss: 0.5620 - val_accuracy: 0.7447 - val_loss: 0.5182\n",
      "Epoch 2/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 79ms/step - accuracy: 0.7490 - loss: 0.5060 - val_accuracy: 0.7529 - val_loss: 0.5063\n",
      "Epoch 3/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 79ms/step - accuracy: 0.7636 - loss: 0.4809 - val_accuracy: 0.7608 - val_loss: 0.4897\n",
      "Epoch 4/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 81ms/step - accuracy: 0.7801 - loss: 0.4501 - val_accuracy: 0.7594 - val_loss: 0.4881\n",
      "Epoch 5/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 81ms/step - accuracy: 0.7901 - loss: 0.4300 - val_accuracy: 0.7633 - val_loss: 0.4908\n",
      "Epoch 6/6\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 83ms/step - accuracy: 0.8021 - loss: 0.4081 - val_accuracy: 0.7574 - val_loss: 0.4950\n",
      "ANN Test — Acc: 0.7654500007629395\n"
     ]
    }
   ],
   "source": [
    "# Build Embedding + Average + Dense\n",
    "embed_dim = 100\n",
    "\n",
    "inp1 = layers.Input(shape=(max_len,))\n",
    "inp2 = layers.Input(shape=(max_len,))\n",
    "\n",
    "emb = layers.Embedding(vocab_size, embed_dim, input_length=max_len)\n",
    "\n",
    "x1 = layers.GlobalAveragePooling1D()(emb(inp1))\n",
    "x2 = layers.GlobalAveragePooling1D()(emb(inp2))\n",
    "\n",
    "x = layers.Concatenate()([x1, x2])\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "ann_model = models.Model([inp1, inp2], out)\n",
    "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor='val_accuracy')\n",
    "hist = ann_model.fit(\n",
    "    [X_train_nn_q1, X_train_nn_q2], y_train_nn,\n",
    "    validation_split=0.1, epochs=6, batch_size=512, callbacks=[es], verbose=1\n",
    ")\n",
    "\n",
    "ann_eval = ann_model.evaluate([X_test_nn_q1, X_test_nn_q2], y_test_nn, verbose=0)\n",
    "print(\"ANN Test — Acc:\", ann_eval[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52751777-eab5-4023-ad8b-d9a46aa50105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 149ms/step - accuracy: 0.7567 - loss: 0.4936 - val_accuracy: 0.7893 - val_loss: 0.4463\n",
      "Epoch 2/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 148ms/step - accuracy: 0.8284 - loss: 0.3734 - val_accuracy: 0.8007 - val_loss: 0.4346\n",
      "Epoch 3/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 152ms/step - accuracy: 0.8820 - loss: 0.2685 - val_accuracy: 0.7999 - val_loss: 0.4861\n",
      "Epoch 4/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 154ms/step - accuracy: 0.9185 - loss: 0.1904 - val_accuracy: 0.7941 - val_loss: 0.5702\n",
      "CNN Test — Acc: 0.8020125031471252\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "\n",
    "inp1 = layers.Input(shape=(max_len,))\n",
    "inp2 = layers.Input(shape=(max_len,))\n",
    "emb = layers.Embedding(vocab_size, embed_dim, input_length=max_len)\n",
    "\n",
    "def cnn_branch(x):\n",
    "    x = emb(x)\n",
    "    x1 = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x1 = layers.GlobalMaxPooling1D()(x1)\n",
    "    x2 = layers.Conv1D(128, 4, activation='relu')(x)\n",
    "    x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "    x3 = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "    x3 = layers.GlobalMaxPooling1D()(x3)\n",
    "    x = layers.Concatenate()([x1, x2, x3])\n",
    "    return x\n",
    "\n",
    "b1 = cnn_branch(inp1)\n",
    "b2 = cnn_branch(inp2)\n",
    "x = layers.Concatenate()([b1, b2])\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "cnn_model = models.Model([inp1, inp2], out)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor='val_accuracy')\n",
    "cnn_model.fit([X_train_nn_q1, X_train_nn_q2], y_train_nn,\n",
    "              validation_split=0.1, epochs=6, batch_size=256, callbacks=[es], verbose=1)\n",
    "\n",
    "cnn_eval = cnn_model.evaluate([X_test_nn_q1, X_test_nn_q2], y_test_nn, verbose=0)\n",
    "print(\"CNN Test — Acc:\", cnn_eval[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f362c387-3a80-485a-b71f-d9d64ee9c4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 324ms/step - accuracy: 0.7497 - loss: 0.5044 - val_accuracy: 0.7745 - val_loss: 0.4678\n",
      "Epoch 2/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 317ms/step - accuracy: 0.8010 - loss: 0.4207 - val_accuracy: 0.7850 - val_loss: 0.4480\n",
      "Epoch 3/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 280ms/step - accuracy: 0.8304 - loss: 0.3647 - val_accuracy: 0.7835 - val_loss: 0.4598\n",
      "Epoch 4/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 256ms/step - accuracy: 0.8529 - loss: 0.3201 - val_accuracy: 0.7916 - val_loss: 0.4753\n",
      "Epoch 5/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 260ms/step - accuracy: 0.8698 - loss: 0.2846 - val_accuracy: 0.7883 - val_loss: 0.5110\n",
      "Epoch 6/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 252ms/step - accuracy: 0.8845 - loss: 0.2543 - val_accuracy: 0.7902 - val_loss: 0.5479\n",
      "BiLSTM Test — Acc: 0.7900000214576721\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "\n",
    "inp1 = layers.Input(shape=(max_len,))\n",
    "inp2 = layers.Input(shape=(max_len,))\n",
    "emb = layers.Embedding(vocab_size, embed_dim, input_length=max_len, mask_zero=False)\n",
    "\n",
    "def lstm_branch(x):\n",
    "    x = emb(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)\n",
    "    return x\n",
    "\n",
    "b1 = lstm_branch(inp1)\n",
    "b2 = lstm_branch(inp2)\n",
    "x = layers.Concatenate()([b1, b2])\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "lstm_model = models.Model([inp1, inp2], out)\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor='val_accuracy')\n",
    "lstm_model.fit([X_train_nn_q1, X_train_nn_q2], y_train_nn,\n",
    "               validation_split=0.1, epochs=6, batch_size=256, callbacks=[es], verbose=1)\n",
    "\n",
    "lstm_eval = lstm_model.evaluate([X_test_nn_q1, X_test_nn_q2], y_test_nn, verbose=0)\n",
    "print(\"BiLSTM Test — Acc:\", lstm_eval[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b662b26-9441-49df-b679-80c1453abdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: model_quora.pkl, tfidf.pkl, cv.pkl, tokenizer.json, ann_model.h5, cnn_model.h5, bilstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Best classical model (example: SVM from grid search)\n",
    "with open('model_quora.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_best, f)\n",
    "\n",
    "with open('tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "with open('cv.pkl', 'wb') as f:\n",
    "    pickle.dump(cv, f)\n",
    "\n",
    "# Save tokenizer & DL models if you want Streamlit to use them later\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "\n",
    "ann_model.save('ann_model.h5')\n",
    "cnn_model.save('cnn_model.h5')\n",
    "lstm_model.save('bilstm_model.h5')\n",
    "\n",
    "print(\"Saved: model_quora.pkl, tfidf.pkl, cv.pkl, tokenizer.json, ann_model.h5, cnn_model.h5, bilstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07c996-640b-488b-b83c-0da2b213fd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910a071-267d-4ecb-ac9a-9ecb18287338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5801a81-2503-47c6-9681-55e960110c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
